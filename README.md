**Introduction**

This project aimed to connect three Raspberry Pi 4’s together in an ad-hoc network and pass weather sensor data between them according to two paradigms: consecutive polling and token ring. My partner, Adam, and I’s implementation of these networked environmental sensing apps are scaled-down demonstrations of popular WSN (wireless sensor network) and IoT (Internet of Things) topologies. The consecutive polling approach represents a heterogeneous network in which one Pi must act as a gateway or datasink, while the token ring approach reflects a homogeneous P2P (peer-to-peer) topology. The foremost problems we solved in our implementations were properly coordinating TCP connections between the Pis and reacting to the dynamic network conditions consequent of mobile nodes in an ad-hoc configuration.

**The Basics: Socket Programming in Python**

multiconn\_ed\_server.py provides a basic implementation of a Python server that can handle multiple simultaneous TCP clients. Its operational logic is event-driven, meaning that it will monitor its various sockets for messages, connection attempts, and closures, altering its internal state accordingly. The low-level system calls that enact the TCP protocol are handled by Python’s socket module.

The accept\_wrapper() function listens on the server socket for incoming connections from a new client using socket.accept(). When one is received, its information is passed to an object from the select module, which will continually monitor the connection for events (simple READ or WRITE requests by the client).

When a client request triggers such an event, it is handled by service\_connection(). If the client wishes to write data, the function will read bytes until they stop being sent, at which point the client is unregistered with the selector object and its TCP connection is closed. Similarly, if the client chooses to read data, the function leaves a pass section in which response message creation and sending coulda be handled. The client is unregistered and the connection is closed after the response is sent. Persistent TCP connections are not possible on this server.

The client end, implemented in multiconn-client.py, has its most interesting code in the start\_connections() function. Here, _n_ TCP connections are iteratively opened with the server located at (host, port). This file also has its own service\_connection() function that is reciprocal to the server side’s implementation. In both the client and server versions of this function, socket.send() and socket.recv() handle passing byte streams to and from the transport layer. When the server sends, the client must receive, and vice-versa.

The prominent difference between this multiple connection server-client setup and a single connection one is the use of non-blocking code to monitor _n_ connections and respond to requests without losing track of other connections. Without the use of the select module in these files, incoming connections to the server would be ignored if the server process was tied up in a socket.recv() call.

**Implementing Polling**

This part of the project implements a heterogeneous topology: two “secondary” Pis register with a “primary” Pi, which aggregates their data along with its own and plots it for viewing. The two secondary Pis have no knowledge of each other and only interact with the primary Pi, while the primary Pi interacts with both and handles opening and closing their respective TCP connections.

Importantly, the primary Pi listens for registration connections from secondary Pis and adds them to a global set of CLIENTS to refer to later. New connections are reported in the terminal upon registration. This process is handled by the server() function, which is called alternately with the more meaty get\_pi\_readings() inside of main(). Also notable is that these two functions are asyncio coroutines, meaning the script uses cooperative multitasking to give the server multi-connection capability. This is an obviously divergent strategy from the multiconn\_ed\_server.py script discussed above–primarily for the ease of reusing code written for a previous project which also compared polling to another paradigm.

Inside get\_pi\_readings(), each client will be sent a message (”Requesting data”) to prompt each secondary Pi to collect sensor measurements, pack them up in a JSON string, and send them back to the primary Pi to be stored in a global-scope JSON object. The connection is closed after successful receipt of the data. If the set CLIENTS has a nonzero length, the primary Pi proceeds to gather its own sensor measurements, store them, and create plots of each Pi’s ambient temperature, wind speed, soil moisture, and humidity, as well as averages of these values.

secondary.py is a generally simpler file. A single command line argument is passed to collect the final byte of the Pi’s IP address. Since our ad-hoc network address is 169.223.1, the final byte is used as the host address. This was initially a workaround for being able to quickly switch whether my or Adam’s Pi was primary, but proved to be a useful feature once we introduced the third Pi. The main() function first calls start(), which registers with the primary Pi by sending a simple message (”Hello, world!”) to it so that its IP address and port number can be stored in its CLIENTS set. The connection is subsequently closed. Within a while True loop, the Pi will listen for connections from the primary Pi requesting its data. Receiving one will let the Pi call each of  its read\_{sensor}() functions and package their results up nicely before sending it to the primary Pi. Timeouts, along with any TCP exceptions, are caught by printing the error to the terminal and calling start() again before reentering the socket.listen() portion. Essentially, the secondary Pis are programmed to reconnect indefinitely.

The major issue my partner and I encountered when testing these scripts was getting the secondary Pis exception block to work correctly. Initially, the block was written such that the original socket would be closed (and the object therefore destroyed) upon error and a new one would be created inside the block using socket.socket(). Doing this generated an “address in use” error, crashing the program and making for sub-satisfactory robustness. We realized the solution was to let the primary Pi close the connection since it is already programmed to do so in get\_pi\_readings(), so we only needed to re-register using start() and begin listening again, letting the primary Pi take care of the rest.

**Implementing Token Ring**

Token rings deal with the problem of medium access control by passing a token (usually a small piece of signalling data) to each node to give it permission to utilize the communication channel. This is dissimilar to a contention-based MAC protocol like CSMA/CA because there _is_ no contention for the channel. It is basically like time division multiple access in that each node must wait for its turn to use the channel bandwidth, except instead of waiting for a certain period of time to talk, it will wait until it receives the token. Sometimes the token is simply implied by the act of receiving data from the preceding node before sending yours to the next node. The topology is therefore cyclical and no node will communicate out of turn ([source](https://en.wikipedia.org/wiki/Token_passing)).

My partner and I’s implementation has a token that is purely symbolic. We choose to prepend ”token” to each packet sent in the ring, but this was only to aid visualization and understanding of the token ring topology.

Since token ring is a P2P-like paradigm, each Pi runs the script token-ring.py. The first important aspect of this program is a global dictionary called CONFIG. It maps integer keys to dictionary values, where the integers (1, 2, 3) are an identifier for each Pi and the dictionary values are themselves dictionaries, in the format prev\_ip:”X.X.X.X”, this\_ip:X.X.X.X, next\_ip:”X.X.X.X”. This data structure is basically a virtual linked list that represents the links in the topology. CONFIG is accessed every time a Pi needs to send or receive a packet. However, its most interesting use is to repair the topology in the case of a single node disconnect and subsequently return to the original topology in the case of reconnection.

The next key piece of the program is the function sense\_and\_marshall(), which is a wrapper for all the sensor access functions as well as a JSON string pack and unpacker. It takes in a JSON string, adds its own sensor data to the various fields, and returns a new, updated JSON string to the caller, so this is where all data aggregation in the network occurs. 

handle\_connection() is where each Pi spends the vast majority of runtime. Indefinitely, the Pi is programmed to listen for messages, check if the first five characters are ”token,” slice the token off to get the JSON string payload, pass it to sense\_and\_marshall(), and send it on to the Pi at the address next\_ip. There are some logical branches sprinkled in to handle the things like the slightly different behaviors of Pi 3 versus 1 and 2 due to the plotting responsibilities of Pi 3 and the need for Pi 3 to send an empty JSON  string to 1 in order to re-initialize the data being passed through the ring. Most importantly for robustness and topology sensitivity, some of these branches check to see if the ring is broken. The address of the sender is compared to the one pointed to by prev\_ip. If it is not, a special field in the JSON object (“RESET”) is changed from 0 to 1. This will be detected inside of sense\_and\_marshall() when the next Pi receives this packet, and change the value of next\_ip to the one stored in CONFIG\[pi\_id % 3 + 1\]\["next\_ip"\]. This change is made inside a call to the function reconfigure().

By far the biggest challenge for token-ring.py was getting the reconfiguration mechanism to work properly. It took a couple of design iterations to get it right. We first attempted to keep a global set variable called NETWORK that stores each known IP in the topology, but we soon realized this was more complex than it needed to be since each Pi already has access to every address in the network via CONFIG. Additionally, we spent a lot of time not realizing that CONFIG was being erroneously edited during reconfigure() calls because the object it manipulates was originally written as a _reference_ to CONFIG and not a copy of it.

**Timeouts**

Both approaches used socket.settimeout() to establish the number of seconds to wait before issuing a socket.timeout exception. For the consecutive polling portion of the project, timeouts are pretty much used as a way to keep communication up between the Pis that _can_ reach each other and skip over trying to communicate with ones that cannot. Inside primary.py’s server(), the exception is caught, but no action is taken. The pass keyword is used to end that block without doing anything. The control flow falls through to await asyncio.sleep(), so that the function will simply be called again from main() due to its use of asyncio.gather(). In get\_pi\_readings(), the same thing happens, but pass is replaced with closing the TCP connection, removing the appropriate address from CLIENTS, and reporting the error to the standard output. secondary.py’s server code functions similarly, but a timeout will prompt the Pi to re-register using the process discussed in the Implementing Consecutive Polling section. It will continually do this until a connection is made because the try:except: block this code lives in is inside of a while True loop.

The token ring approach uses timeouts in a much more interesting way. If a node is unable to send its packet to the next in the ring due to timeout or connection refusal ten or more times, the topology is reconfigured. Additionally, if a Pi’s call to socket.listen() to the previous one times out, a blank-slate JSON object with the RESET field set to 1 is sent to the Pi ahead in order to keep data flowing and trigger reconfiguration.

Detecting and reacting to timeouts proved helpful in making our two networks more resilient, but there are certainly still cases in which they could be rendered ineffective. For the token ring network, there is a period during which a disconnected Pi cannot try to reconnect for risk of confusing the other two Pis. Since the topology reconfiguration takes 1 RTT to propagate to both of the remaining nodes in the network but the Pis are almost always listening for messages on their sockets, a dropout followed by a reconnect that takes less than 1 RTT would very much confuse at least one of the Pis. This is accounted for by the 10 retries required to force sending data downstream, but I’m sure it is possible for there to be some order of events that slips through the cracks. 

**Consecutive Polling vs. Token Ring**

The two approaches explored in this project both have benefits and drawbacks. Our token ring implementation is superior in robustness since the network can reactively adjust to changes and any Pi can fill any role, as long as the host addresses inside CONFIG correspond to the number passed as in the command line by the user. Even if a Pi drops out, the others can continue collecting data and graphing it, assuming Pi 3 is present. However, latency is worse than the heterogeneous network because a full cycle of the ring must pass before data can be averaged and graphed. The heterogeneous network takes advantage of TCP’s full duplex nature because if the two secondary Pis send their data at nearly the same time, the primary one can receive the others’ packets, capture its own sensor data and plot all of them in a time theoretically close to 0.5 RTT. This best-case latency is hugely favorable compared to the homogenous token ring’s best-case time of 1.5 RTT (1 -> 2, 2 -> 3, 3 -> 1).

Therefore, the token ring approach is likely better suited to situations in which data freshness and speed are not of great importance but reactivity is desirable. I can imagine that a larger IoT network in which many or all nodes are mobile and the nodes possibly actuate a physical mechanism or contribute non-time sensitive sensor measurements to a database would benefit from token rings. Meanwhile, the polling approach is suited for a topology in which there is a stationary gateway node that possibly mobile nodes could move in and out of range from, reporting their fresh data when they can. It reminds me of projects Prof. Obraczka described in class in which animals carrying sensor data might move in and out of range of a data sink. When they are in range, fresh data from nearby sensors can be fed to the gateway with low latency. It is effectively a best-effort model.

**Conclusion**

This project forced me to think through a lot of design decisions one faces when programming networks that are not just capable of but _likely to_ change. It always amazed me that protocols like WiFi (when they work) can so easily adjust to members coming and going and that these adjustments can happen so quickly. Although I was already familiar with socket programming in Python from CSE 150, it was invaluable to refresh my knowledge of it and solidify the best practices for creating a stable, nonblocking, multiple connection-enabled server. My partner and I are on a team of only two people, so it was also a great opportunity to learn from one another and cooperatively debug issues that each of us were facing, as well as collaborate on implementations and work through high-level designs before moving to the IDE.
